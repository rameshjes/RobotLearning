Example: Consider a Markov Decision Process example whether a door is open or closed


States : {Door open, Door closed}
actions: {opening door, closing door}
Transition probabilities: What is probability if action "opening door" is performed, door gets open, or what is probability when "closing door" is performed, door gets close.
Reward: If door is open, we can get positive/high reward , if it is closed, we get negative/low reward


Now, if we destroy one state, and consider one state: Door open, then we cannot determine completely as there can be chances that door can be closed. In this case, MDP is violated as states are not completely defined. 
