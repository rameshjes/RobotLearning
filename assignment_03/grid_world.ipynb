{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        # create an array for grid \n",
    "        # 0 represents empty cell, 1 represents *, and 2 represents x\n",
    "        self.grid = np.array([[0,0,0,0,0,0,0,0,0],\n",
    "                              [0,1,1,1,1,2,0,0,0],\n",
    "                              [0,2,2,2,0,2,2,0,0],\n",
    "                              [0,0,0,0,0,0,0,2,0],\n",
    "                              [0,0,2,2,2,2,0,2,0],\n",
    "                              [0,0,0,0,0,0,0,2,0],\n",
    "                              [0,2,2,2,0,2,2,0,0],\n",
    "                              [0,0,0,0,0,0,0,0,3],\n",
    "                              [0,1,1,1,2,1,1,0,2]]) \n",
    "        self.rewards = {0:-1, 1:5, 2:-20, 3:100} # rewards \n",
    "        self.actions = np.array([[1, 0.125],[2, 0.125], [3, 0.625],[4, 0.125]]) #up,right,down,left with probabilities\n",
    "        self.transition_probability = np.array([0.2,0.6,0.2]) # left diagonal, desired direction, right diagonal\n",
    "        self.gama = 0.9  # discount factor\n",
    "        self.threshold = 0.0001  # theta\n",
    "        self.n=9  # number of rows \n",
    "        self.a=4  # number of actions\n",
    "        self.states = self.n*self.n\n",
    "\n",
    "    def get_next_state(self,row,col,a):\n",
    "        # going up\n",
    "        if a == 1:\n",
    "            # if state is in row 0 it stays in same row\n",
    "            if row == 0:\n",
    "                r = row\n",
    "                c = col\n",
    "                reward = -10\n",
    "            else:\n",
    "                # if next state is not reachable\n",
    "                reward = self.rewards[self.grid[row-1,col]]\n",
    "                if self.grid[row-1,col] == 2:\n",
    "                    r = row\n",
    "                    c = col\n",
    "                # if next state is reachable and no obstacle\n",
    "                else:\n",
    "                    r = row-1\n",
    "                    c = col\n",
    "        # go right\n",
    "        elif a == 2:\n",
    "            # if state is in last column it stays in same column\n",
    "            if col == 8:\n",
    "                r = row\n",
    "                c = col\n",
    "                reward = -10\n",
    "            else:\n",
    "                # if next state is not reachable\n",
    "                reward = self.rewards[self.grid[row,col+1]]\n",
    "                if self.grid[row,col+1] == 2:\n",
    "                    r = row\n",
    "                    c = col\n",
    "                # if next state is reachable and no obstacle\n",
    "                else:\n",
    "                    r = row\n",
    "                    c = col+1\n",
    "        # go down\n",
    "        elif a == 3:\n",
    "            # if state is in row 8 it stays in same row\n",
    "            if row == 8:\n",
    "                r = row\n",
    "                c = col\n",
    "                reward = -10\n",
    "            else:\n",
    "                # if next state is not reachable\n",
    "                reward = self.rewards[self.grid[row+1,col]]\n",
    "                if self.grid[row+1,col] == 2:\n",
    "                    r = row\n",
    "                    c = col\n",
    "                # if next state is reachable and no obstacle\n",
    "                else:\n",
    "                    r = row+1\n",
    "                    c = col\n",
    "        # go left\n",
    "        elif a == 4:\n",
    "            # if state is in first column it stays in same column\n",
    "            if col == 0:\n",
    "                r = row\n",
    "                c = col\n",
    "                reward = -10\n",
    "            else:\n",
    "                # if next state is not reachable\n",
    "                reward = self.rewards[self.grid[row,col-1]]\n",
    "                if self.grid[row,col-1] == 2:\n",
    "                    r = row\n",
    "                    c = col\n",
    "                # if next state is reachable and no obstacle\n",
    "                else:\n",
    "                    r = row\n",
    "                    c = col-1\n",
    "        return r,c,reward\n",
    "    \n",
    "    # states with undeterministic action\n",
    "    # 0.6 probability to move in desired direction, 0.2 to move left diagonal, 0.2 for right diagonal\n",
    "    def get_next_non_determinstic_state(self,row,col,a):\n",
    "        self.possible_states = np.zeros((3,4))\n",
    "        # going up\n",
    "        if a == 1:\n",
    "            for i in range(3):\n",
    "                # going up,Diagonaly left\n",
    "                if i == 0:\n",
    "                    if row == 0:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if col == 0:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row-1,col-1]]\n",
    "                            if self.grid[row-1,col] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row-1\n",
    "                                c = col-1\n",
    "                    prob = self.transition_probability[i]\n",
    "                #going up, Desired direction\n",
    "                if i == 1:\n",
    "                    if row == 0:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        reward = self.rewards[self.grid[row-1,col]]\n",
    "                        if self.grid[row-1,col] == 2:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                        # if next state is reachable and no obstacle\n",
    "                        else:\n",
    "                            r = row-1\n",
    "                            c = col\n",
    "                    prob = self.transition_probability[i]\n",
    "                # going up,Diagonally right\n",
    "                if i == 2:\n",
    "                    if row == 0:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if col == 8:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row-1,col+1]]\n",
    "                            if self.grid[row-1,col+1] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row-1\n",
    "                                c = col+1\n",
    "                    prob = self.transition_probability[i]\n",
    "                self.possible_states[i,:] = np.array([r,c,reward,prob])\n",
    "        # going right \n",
    "        if a == 2:\n",
    "            for i in range(3):\n",
    "                # going right, Diagonaly left\n",
    "                if i == 0:\n",
    "                    if col == 8:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if row == 0:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row-1,col+1]]\n",
    "                            if self.grid[row-1,col+1] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row-1\n",
    "                                c = col+1\n",
    "                    prob = self.transition_probability[i]\n",
    "                # going right,Desired direction\n",
    "                if i == 1:\n",
    "                    if col == 8:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        reward = self.rewards[self.grid[row,col+1]]\n",
    "                        if self.grid[row,col+1] == 2:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                        # if next state is reachable and no obstacle\n",
    "                        else:\n",
    "                            r = row\n",
    "                            c = col+1\n",
    "                    prob = self.transition_probability[i]\n",
    "                # going right,Diagonally right\n",
    "                if i == 2:\n",
    "                    if col == 8:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if row == 8:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row+1,col+1]]\n",
    "                            if self.grid[row+1,col+1] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row+1\n",
    "                                c = col+1\n",
    "                    prob = self.transition_probability[i]\n",
    "                self.possible_states[i,:] = np.array([r,c,reward,prob])\n",
    "        # going down\n",
    "        if a == 3:\n",
    "            for i in range(3):\n",
    "                # going down,  Diagonaly left\n",
    "                if i == 0:\n",
    "                    if row == 8:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if col == 8:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row+1,col+1]]\n",
    "                            if self.grid[row+1,col+1] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row+1\n",
    "                                c = col+1\n",
    "                    prob = self.transition_probability[i]\n",
    "                # going down, Desired direction\n",
    "                if i == 1:\n",
    "                    if row == 8:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        reward = self.rewards[self.grid[row+1,col]]\n",
    "                        if self.grid[row+1,col] == 2:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                        # if next state is reachable and no obstacle\n",
    "                        else:\n",
    "                            r = row+1\n",
    "                            c = col\n",
    "                    prob = self.transition_probability[i]\n",
    "                # going down, Diagonally right\n",
    "                if i == 2:\n",
    "                    if row == 8:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if col == 0:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row+1,col-1]]\n",
    "                            if self.grid[row+1,col-1] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row+1\n",
    "                                c = col-1\n",
    "                    prob = self.transition_probability[i]\n",
    "                self.possible_states[i,:] = np.array([r,c,reward,prob])\n",
    "        # going left \n",
    "        if a == 4:\n",
    "            for i in range(3):\n",
    "                # going left,  Diagonaly left\n",
    "                if i == 0:\n",
    "                    if col == 0:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if row == 8:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row+1,col-1]]\n",
    "                            if self.grid[row+1,col-1] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row+1\n",
    "                                c = col-1\n",
    "                    prob = self.transition_probability[i]\n",
    "                # going left, Desired direction\n",
    "                if i == 1:\n",
    "                    if col == 0:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        reward = self.rewards[self.grid[row,col-1]]\n",
    "                        if self.grid[row,col-1] == 2:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                        # if next state is reachable and no obstacle\n",
    "                        else:\n",
    "                            r = row\n",
    "                            c = col-1\n",
    "                    prob = self.transition_probability[i]\n",
    "                # going left, Diagonally right\n",
    "                if i == 2:\n",
    "                    if col == 0:\n",
    "                        r = row\n",
    "                        c = col\n",
    "                        reward = -10\n",
    "                    else:\n",
    "                        if row == 0:\n",
    "                            r = row\n",
    "                            c = col\n",
    "                            reward = -10\n",
    "                        else:\n",
    "                            reward = self.rewards[self.grid[row-1,col-1]]\n",
    "                            if self.grid[row-1,col-1] == 2:\n",
    "                                r = row\n",
    "                                c = col\n",
    "                            # if next state is reachable and no obstacle\n",
    "                            else:\n",
    "                                r = row-1\n",
    "                                c = col-1\n",
    "                    prob = self.transition_probability[i]\n",
    "                self.possible_states[i,:] = np.array([r,c,reward,prob])\n",
    "        return self.possible_states\n",
    "        \n",
    "    # get the expected reward of state\n",
    "    def get_value(self,row,col):\n",
    "        index = row*9 + col\n",
    "        return self.V[index]\n",
    "    \n",
    "    # get the values of next state\n",
    "    def get_next_state_value(self, row, col):\n",
    "        A_v = np.zeros(self.a)\n",
    "        for j in range(1,5): # for each action\n",
    "            r,c,reward = self.get_next_state(row,col,j) # get next state and reward\n",
    "            next_s = r*9 + c\n",
    "            next_v = self.get_value(r,c) # get value from next state\n",
    "            prob = self.actions[j-1,1] # probability of action\n",
    "            A_v[j-1]  += prob*(reward + self.gama*next_v)\n",
    "        return A_v\n",
    "    \n",
    "    # get the value of non-deterministic states\n",
    "    def get_next_non_deterministic_state_value(self, row, col):\n",
    "        A_v = np.zeros(self.a)\n",
    "        for j in range(1,5): # for each action\n",
    "            possible_states = self.get_next_non_determinstic_state(row,col,j) # get next state and reward\n",
    "            for s in range(3):\n",
    "                next_s = possible_states[s,0]*9 + possible_states[s,1]\n",
    "                next_v = self.get_value(possible_states[s,0],possible_states[s,1]) # get value from nest state\n",
    "                prob = self.actions[j-1,1] # probability of action\n",
    "                A_v[j-1]  +=  possible_states[s,3]*prob*(possible_states[s,2] + self.gama*next_v)\n",
    "        return A_v\n",
    "    \n",
    "    # evaluate the policy\n",
    "    def policy_evaluation(self):        \n",
    "        self.V = np.zeros(self.states)\n",
    "        count = 0\n",
    "        iterate = True\n",
    "        while iterate:\n",
    "        \n",
    "            delta = 0\n",
    "            for i in range(self.states):\n",
    "                row = int(i / self.n)\n",
    "                col = i % self.n\n",
    "                s = self.grid[row,col]\n",
    "                v = 0\n",
    "                if s != 2 and s != 3:\n",
    "                    for j in range(1,5): # for each action\n",
    "                        r,c,reward = self.get_next_state(row,col,j) # get next state and reward\n",
    "                        next_s = r*9 + c\n",
    "                        next_v = self.get_value(r,c) # get value from nest state\n",
    "                        prob = self.actions[j-1,1] # probability of action\n",
    "                        v  += prob*(reward + self.gama*next_v)\n",
    "        \n",
    "                    delta = max(delta, np.abs(v - self.V[i]))\n",
    "                self.V[i] = v\n",
    "            count += 1\n",
    "            # if delta is smaller than certain threshold, it stops.\n",
    "            if delta < self.threshold:\n",
    "                break\n",
    "        return self.V, delta, count\n",
    "\n",
    "    # value iteration\n",
    "    def value_iteration(self):        \n",
    "        self.V = np.zeros(self.states)\n",
    "        self.policy = np.zeros((self.states,self.a))\n",
    "        count = 0\n",
    "        iterate = True\n",
    "        while iterate:\n",
    "        #     print(count)\n",
    "            delta = 0\n",
    "            for i in range(self.states):\n",
    "                row = int(i / self.n)\n",
    "                col = i % self.n\n",
    "                s = self.grid[row,col]\n",
    "                best_v = 0\n",
    "                # if state is either * or x, then values of those state remain same\n",
    "                if s != 2 and s != 3:\n",
    "                    values = self.get_next_state_value(row,col)\n",
    "                    best_v = max(values)\n",
    "                    delta = max(delta, np.abs(best_v- self.V[i]))\n",
    "                self.V[i] = best_v\n",
    "            count += 1\n",
    "            if delta < self.threshold:\n",
    "                break\n",
    "        \n",
    "        for i in range(self.states):\n",
    "            row = int(i / self.n)\n",
    "            col = i % self.n\n",
    "            values = self.get_next_state_value(row,col)\n",
    "            # take action with maximum value\n",
    "            best_v = np.argmax(values)\n",
    "            self.policy[i,best_v] = 1\n",
    "        # Deterministic policy     \n",
    "        return self.V, delta, count, self.policy\n",
    "\n",
    "    # non-deterministic actions\n",
    "    def non_deterministic_value_iteration(self):        \n",
    "        self.V = np.zeros(self.states)\n",
    "        self.policy = np.zeros((self.states,self.a))\n",
    "        count = 0\n",
    "        iterate = True\n",
    "        while iterate:\n",
    "        \n",
    "            delta = 0\n",
    "            for i in range(self.states):\n",
    "                row = int(i / self.n)\n",
    "                col = i % self.n\n",
    "                s = self.grid[row,col]\n",
    "                best_v = 0\n",
    "                if s != 2 and s != 3:\n",
    "                    values = self.get_next_non_deterministic_state_value(row,col)\n",
    "                    best_v = max(values)\n",
    "                    delta = max(delta, np.abs(best_v- self.V[i]))\n",
    "                self.V[i] = best_v\n",
    "            count += 1\n",
    "            if delta < self.threshold:\n",
    "                break\n",
    "        # Deterministic policy\n",
    "        for i in range(self.states):\n",
    "            row = int(i / self.n)\n",
    "            col = i % self.n\n",
    "            values = self.get_next_state_value(row,col)\n",
    "            best_v = np.argmax(values)\n",
    "            self.policy[i,best_v] = 1\n",
    "             \n",
    "        return self.V, delta, count, self.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_world = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the expected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('delta: ', 8.8468431698629502e-05)\n",
      "('number of iterations: ', 91)\n",
      "('V:', array([[ -50.1324936 ,  -64.58247582,  -72.65010404,  -76.66298262,\n",
      "         -80.8102371 ,  -99.15175631,  -81.18403045,  -50.63043609,\n",
      "         -20.12029228],\n",
      "       [ -50.3772763 ,  -80.22926875,  -89.26562608,  -93.15408361,\n",
      "         -95.22664654,    0.        ,  -94.35615829,  -55.8449427 ,\n",
      "         -11.81743254],\n",
      "       [ -50.96744537,    0.        ,    0.        ,    0.        ,\n",
      "        -110.7870577 ,    0.        ,    0.        ,  -66.1413208 ,\n",
      "           0.32553778],\n",
      "       [ -52.14645007,  -76.29940005, -118.74797657, -131.19534114,\n",
      "        -124.70584532, -128.54082796, -111.07930378,    0.        ,\n",
      "          19.88315053],\n",
      "       [ -53.04445287,  -80.20470504,    0.        ,    0.        ,\n",
      "           0.        ,    0.        , -117.11229547,    0.        ,\n",
      "          35.32945071],\n",
      "       [ -53.44443734,  -94.67650531, -120.093197  , -116.70428329,\n",
      "         -81.49735801, -119.75857851, -128.91671857,    0.        ,\n",
      "          52.69949744],\n",
      "       [ -51.0015617 ,    0.        ,    0.        ,    0.        ,\n",
      "         -75.29221383,    0.        ,    0.        ,  -14.75467424,\n",
      "          73.54230193],\n",
      "       [ -51.58001462,  -46.34092771,  -46.60471602,  -55.31555955,\n",
      "         -77.21430575,  -55.22588857,  -44.45109547,  -24.81493291,    0.        ],\n",
      "       [ -58.13560721,  -54.14548168,  -53.86732506,  -63.17858985,\n",
      "           0.        ,  -63.46781606,  -54.7924953 ,  -52.94097625,    0.        ]]))\n"
     ]
    }
   ],
   "source": [
    "V,delta, count = grid_world.policy_evaluation()\n",
    "V.shape = (9,9)\n",
    "print('delta: ',delta)\n",
    "print('number of iterations: ',count)\n",
    "print('V:', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Value iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('delta:', 1.6074364062035329e-12)\n",
      "('number of iterations:', 9)\n",
      "('value:', array([[  2.71126761e-01,   3.52112676e+00,   3.52112676e+00,\n",
      "          3.52112676e+00,   3.52112676e+00,   2.71126761e-01,\n",
      "         -9.44982394e-02,  -5.78990933e-02,   5.96452504e-01],\n",
      "       [  7.04225352e-01,   7.04225352e-01,   7.04225352e-01,\n",
      "          7.04225352e-01,   7.04225352e-01,   0.00000000e+00,\n",
      "         -1.11579819e-01,   1.19290501e-01,   2.17147112e+00],\n",
      "       [ -4.57746479e-02,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   7.04225352e-01,   0.00000000e+00,\n",
      "          0.00000000e+00,   4.34294224e-01,   4.97150421e+00],\n",
      "       [ -1.30149648e-01,  -1.39641835e-01,  -1.39641835e-01,\n",
      "         -1.30149648e-01,  -4.57746479e-02,  -1.30149648e-01,\n",
      "         -1.39641835e-01,   0.00000000e+00,   9.94934082e+00],\n",
      "       [ -1.39641835e-01,  -1.40709706e-01,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         -1.40709706e-01,   0.00000000e+00,   1.87988281e+01],\n",
      "       [ -1.40709706e-01,  -1.40829842e-01,  -1.40843357e-01,\n",
      "         -1.40844878e-01,  -1.40844878e-01,  -1.40843357e-01,\n",
      "         -1.40829842e-01,   0.00000000e+00,   3.45312500e+01],\n",
      "       [ -1.40829842e-01,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,  -1.40845049e-01,   0.00000000e+00,\n",
      "          0.00000000e+00,   6.90625000e+00,   6.25000000e+01],\n",
      "       [  2.71126761e-01,   3.52112676e+00,   3.52112676e+00,\n",
      "          3.52112676e+00,   2.71126761e-01,   3.52112676e+00,\n",
      "          3.52112676e+00,   1.25000000e+01,   0.00000000e+00],\n",
      "       [  7.04225352e-01,   7.04225352e-01,   7.04225352e-01,\n",
      "          7.04225352e-01,   0.00000000e+00,   7.04225352e-01,\n",
      "          7.04225352e-01,   1.28125000e+00,   0.00000000e+00]]))\n",
      "\n",
      "0 = up, 1 = right, 2 = down, 3 = left \n",
      "\n",
      "[[1 2 2 2 2 3 3 1 2]\n",
      " [1 1 1 1 3 3 1 1 2]\n",
      " [0 0 0 0 0 3 1 1 2]\n",
      " [0 3 1 1 0 3 3 1 2]\n",
      " [0 0 0 0 0 0 0 1 2]\n",
      " [0 0 3 3 1 1 0 1 2]\n",
      " [0 2 2 2 0 2 2 1 2]\n",
      " [1 2 2 2 3 2 2 1 0]\n",
      " [1 1 1 3 1 1 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "V, delta, count, policy = grid_world.value_iteration()\n",
    "V.shape = (9,9)\n",
    "print('delta:', delta)\n",
    "print('number of iterations:', count)\n",
    "print('value:', V )\n",
    "print(\"\\n0 = up, 1 = right, 2 = down, 3 = left \\n\")\n",
    "print(np.reshape(np.argmax(policy,axis=1),(9,9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Non-deterministic actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('delta:', 2.4178113538830814e-05)\n",
      "('number of iterations:', 12)\n",
      "('V: ', array([[ -3.44243869e-02,   2.42739026e+00,   3.24868693e+00,\n",
      "          3.25252722e+00,   2.74076201e-02,  -2.03966779e-01,\n",
      "         -3.88366809e-01,  -3.88262459e-01,  -3.88366809e-01],\n",
      "       [ -8.99025689e-02,   1.11169750e-01,   2.22084511e-01,\n",
      "          3.22016055e-01,  -5.45573757e-02,   0.00000000e+00,\n",
      "         -1.47190863e-01,  -1.52037335e-01,  -1.47190863e-01],\n",
      "       [ -2.08252792e-01,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   3.64474515e-03,   0.00000000e+00,\n",
      "          0.00000000e+00,  -1.41886109e-01,  -3.71719922e-01],\n",
      "       [ -6.29541091e-01,  -1.75401765e-01,  -6.29541091e-01,\n",
      "         -6.89650886e-01,  -1.09949256e+00,  -6.89650886e-01,\n",
      "         -1.17439941e+00,   0.00000000e+00,  -3.86990826e-01],\n",
      "       [ -1.43202384e-01,  -1.57191006e-01,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         -7.10781693e-01,   0.00000000e+00,   1.56032098e+00],\n",
      "       [ -3.71563129e-01,  -1.62020042e-01,  -6.28617034e-01,\n",
      "         -7.00388340e-01,  -1.17515834e+00,  -7.00388340e-01,\n",
      "         -1.14882636e+00,   0.00000000e+00,   1.58051977e+01],\n",
      "       [ -3.87443440e-01,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,  -2.35840663e-01,   0.00000000e+00,\n",
      "          0.00000000e+00,   1.50495301e+01,   4.17304913e+01],\n",
      "       [ -2.84948271e-01,   2.47737750e+00,   3.22556521e+00,\n",
      "          9.35091627e-02,  -4.49084246e-01,   1.01786676e-01,\n",
      "          2.81863949e+00,   8.09609827e+00,   0.00000000e+00],\n",
      "       [  1.72338870e-01,   1.88448175e-01,   1.72338870e-01,\n",
      "          1.88448175e-01,   0.00000000e+00,   1.84424402e-01,\n",
      "          2.49710577e-01,   3.00990602e+00,   0.00000000e+00]]))\n",
      "\n",
      "0 = up, 1 = right, 2 = down, 3 = left \n",
      "\n",
      "[[1 2 2 2 2 3 3 3 3]\n",
      " [1 1 1 3 3 3 1 1 3]\n",
      " [0 0 0 0 0 3 1 0 3]\n",
      " [1 1 3 3 0 3 3 0 2]\n",
      " [1 3 3 0 0 0 0 1 2]\n",
      " [0 0 3 3 3 1 3 2 2]\n",
      " [0 2 2 1 0 3 1 1 2]\n",
      " [1 2 2 2 1 2 2 1 0]\n",
      " [1 1 1 3 3 1 3 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramesh/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:351: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "V, delta, count, policy = grid_world.non_deterministic_value_iteration()\n",
    "V.shape = (9,9)\n",
    "print('delta:', delta)\n",
    "print('number of iterations:', count)\n",
    "print('V: ', V)\n",
    "print(\"\\n0 = up, 1 = right, 2 = down, 3 = left \\n\")\n",
    "print(np.reshape(np.argmax(policy,axis=1),(9,9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
